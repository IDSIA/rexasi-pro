{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "38ne_mGYXczz",
        "outputId": "ec736846-328b-4e9d-fa32-b1440bb89070"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'yolov5'...\n",
            "remote: Enumerating objects: 16413, done.\u001b[K\n",
            "remote: Counting objects: 100% (5/5), done.\u001b[K\n",
            "remote: Compressing objects: 100% (5/5), done.\u001b[K\n",
            "remote: Total 16413 (delta 0), reused 4 (delta 0), pack-reused 16408\u001b[K\n",
            "Receiving objects: 100% (16413/16413), 14.95 MiB | 24.14 MiB/s, done.\n",
            "Resolving deltas: 100% (11260/11260), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/ultralytics/yolov5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FFuaAFtTXg7A",
        "outputId": "ff6c9a96-92b8-45d1-92cd-ed704f992979"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/yolov5\n"
          ]
        }
      ],
      "source": [
        "%cd yolov5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8l20_q4lXj86",
        "outputId": "e8085248-7861-4b58-8a7f-cac495219fed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.4/196.4 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m709.4/709.4 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -qr requirements.txt  # install dependencies (ignore errors)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t10I-Xt4XmEG",
        "outputId": "d74718e3-5114-4d11-d163-a2dfd7628d48"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mdetect: \u001b[0mweights=['/path/best.pt'], source=/content/test/images, data=data/coco128.yaml, imgsz=[1024, 1024], conf_thres=0.4, iou_thres=0.45, max_det=1000, device=, view_img=False, save_txt=True, save_csv=False, save_conf=False, save_crop=False, nosave=False, classes=None, agnostic_nms=False, augment=False, visualize=False, update=False, project=runs/detect, name=exp, exist_ok=False, line_thickness=3, hide_labels=False, hide_conf=False, half=False, dnn=False, vid_stride=1\n",
            "YOLOv5 🚀 v7.0-283-g875d9278 Python-3.10.12 torch-2.1.0+cu121 CPU\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/yolov5/detect.py\", line 309, in <module>\n",
            "    main(opt)\n",
            "  File \"/content/yolov5/detect.py\", line 304, in main\n",
            "    run(**vars(opt))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/content/yolov5/detect.py\", line 115, in run\n",
            "    model = DetectMultiBackend(weights, device=device, dnn=dnn, data=data, fp16=half)\n",
            "  File \"/content/yolov5/models/common.py\", line 370, in __init__\n",
            "    model = attempt_load(weights if isinstance(weights, list) else w, device=device, inplace=True, fuse=fuse)\n",
            "  File \"/content/yolov5/models/experimental.py\", line 78, in attempt_load\n",
            "    ckpt = torch.load(attempt_download(w), map_location=\"cpu\")  # load\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/serialization.py\", line 986, in load\n",
            "    with _open_file_like(f, 'rb') as opened_file:\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/serialization.py\", line 435, in _open_file_like\n",
            "    return _open_file(name_or_buffer, mode)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/serialization.py\", line 416, in __init__\n",
            "    super().__init__(open(name, mode))\n",
            "FileNotFoundError: [Errno 2] No such file or directory: '/path/best.pt'\n"
          ]
        }
      ],
      "source": [
        "!python detect.py --weights /path/best.pt --img 1024 --conf 0.4 --source /content/test/images --save-txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L_zxe_yyXqBS",
        "outputId": "906ace0c-bf0e-44a1-9bd5-1ff081311713"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-02-08 09:10:31--  https://raw.githubusercontent.com/UjjwalSaxena/Automold--Road-Augmentation-Library/master/Helpers.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2995 (2.9K) [text/plain]\n",
            "Saving to: ‘Helpers.py’\n",
            "\n",
            "\rHelpers.py            0%[                    ]       0  --.-KB/s               \rHelpers.py          100%[===================>]   2.92K  --.-KB/s    in 0s      \n",
            "\n",
            "2024-02-08 09:10:31 (38.9 MB/s) - ‘Helpers.py’ saved [2995/2995]\n",
            "\n",
            "--2024-02-08 09:10:31--  https://raw.githubusercontent.com/UjjwalSaxena/Automold--Road-Augmentation-Library/master/Automold.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 31535 (31K) [text/plain]\n",
            "Saving to: ‘Automold.py’\n",
            "\n",
            "Automold.py         100%[===================>]  30.80K  --.-KB/s    in 0.002s  \n",
            "\n",
            "2024-02-08 09:10:32 (15.5 MB/s) - ‘Automold.py’ saved [31535/31535]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/UjjwalSaxena/Automold--Road-Augmentation-Library/master/Helpers.py\n",
        "!wget https://raw.githubusercontent.com/UjjwalSaxena/Automold--Road-Augmentation-Library/master/Automold.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g61_LvPKXrj5"
      },
      "outputs": [],
      "source": [
        "import Automold as am\n",
        "import Helpers as hp\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0p2uSXFwX36b"
      },
      "outputs": [],
      "source": [
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VHajsPrdX5kT",
        "outputId": "b6023c3f-9797-47fa-e716-139cd1de0fae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.1.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.9.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "89Tz80ToX8LC",
        "outputId": "b15ae8f1-8ba4-4a0d-d1fc-830d67085d1f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/hub.py:294: UserWarning: You are about to download and run code from an untrusted repository. In a future release, this won't be allowed. To add the repository to your trusted list, change the command to {calling_fn}(..., trust_repo=False) and a command prompt will appear asking for an explicit confirmation of trust, or load(..., trust_repo=True), which will assume that the prompt is to be answered with 'yes'. You can also use load(..., trust_repo='check') which will only prompt for confirmation if the repo is not already trusted. This will eventually be the default behaviour\n",
            "  warnings.warn(\n",
            "Downloading: \"https://github.com/ultralytics/yolov5/zipball/master\" to /root/.cache/torch/hub/master.zip\n",
            "YOLOv5 🚀 2024-2-8 Python-3.10.12 torch-2.1.0+cu121 CPU\n",
            "\n",
            "Downloading https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5s.pt to yolov5s.pt...\n",
            "100%|██████████| 14.1M/14.1M [00:00<00:00, 90.1MB/s]\n",
            "\n",
            "Fusing layers... \n",
            "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients, 16.4 GFLOPs\n",
            "Adding AutoShape... \n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "\n",
        "# Load the pretrained YOLO model\n",
        "model = torch.hub.load('ultralytics/yolov5', 'yolov5s')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VHop1fRZYAPp"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import os\n",
        "\n",
        "video_path = 'rosbag2_2023_06_14-14_47_56__rm_drone__camera__image_h264.mp4'\n",
        "cap = cv2.VideoCapture(video_path)\n",
        "output_dir = 'output_frames_d_12'\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "frame_count = 0\n",
        "while True:\n",
        "    # Read a frame from the video\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        # Break the loop if no more frames are available\n",
        "        break\n",
        "\n",
        "    # Save the frame as an image\n",
        "    output_path = os.path.join(output_dir, f'frame_{frame_count:04d}.png')\n",
        "    cv2.imwrite(output_path, frame)\n",
        "\n",
        "    frame_count += 1\n",
        "\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "nk6y9lngxLa-",
        "outputId": "bd361461-fc0e-4039-85b7-01a5bb39e418"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/yolov5/output_frames_d_1.zip'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "import shutil\n",
        "\n",
        "# Replace 'output_frames' with the name of the folder you want to download\n",
        "folder_name = 'output_frames_d_1'\n",
        "\n",
        "# Create a zip archive of the folder\n",
        "shutil.make_archive(folder_name, 'zip', folder_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X85oSrTkYLCY"
      },
      "source": [
        "# Vanilla"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "viqZDNk-YDRm"
      },
      "outputs": [],
      "source": [
        "list_of_distances = []\n",
        "list_of_widths = []\n",
        "# Function to compute distance from bounding box\n",
        "def compute_distance(predictions):\n",
        "    focal_length = 2400.0  # Focal length in pixels\n",
        "    object_width = 0.24  # Object width in meters\n",
        "    bbox_widths = []\n",
        "    distances = []\n",
        "    for pred in predictions:\n",
        "        x1, y1, x2, y2 = pred[:4]\n",
        "        class_index = int(pred[5])\n",
        "        class_label = model.names[class_index]\n",
        "        if class_label == 'motorcycle':\n",
        "          bbox_width = abs(x2 - x1)\n",
        "          bbox_widths.append(bbox_width)\n",
        "          distances.append((object_width * focal_length) / bbox_width)\n",
        "    return distances, bbox_widths\n",
        "\n",
        "# Replace 'output_frames' with the name of the folder containing frames\n",
        "input_folder = 'output_frames_d_12'\n",
        "\n",
        "# List all frame files in the input folder\n",
        "frame_files = [file for file in os.listdir(input_folder) if file.endswith('.png')]\n",
        "\n",
        "# Loop through each frame file\n",
        "for i in range(len(frame_files)):\n",
        "    # Create the full path of the image\n",
        "    image_path = os.path.join(input_folder, f'frame_{i:04d}.png')\n",
        "\n",
        "    # Open the image\n",
        "    image = Image.open(image_path)\n",
        "\n",
        "    # Run inference on the model\n",
        "    results = model(image)\n",
        "\n",
        "    # Extract the bounding box predictions as a NumPy array\n",
        "    predictions = results.xyxy[0].numpy()\n",
        "\n",
        "    # Compute distances for each frame and print the results\n",
        "    distances, widths = compute_distance(predictions)\n",
        "    if len(distances) > 0:\n",
        "      list_of_distances.append(distances[0])\n",
        "      list_of_widths.append(widths[0])\n",
        "    else:\n",
        "      list_of_distances.append(np.nan)\n",
        "      list_of_widths.append(np.nan)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ew31QmS3YI5H"
      },
      "source": [
        "# Bright"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LNhU33hbYHqq"
      },
      "outputs": [],
      "source": [
        "list_of_distances_b = []\n",
        "list_of_widths_b = []\n",
        "# Function to compute distance from bounding box\n",
        "def compute_distance(predictions):\n",
        "    focal_length = 2400.0  # Focal length in pixels\n",
        "    object_width = 0.24  # Object width in meters\n",
        "    bbox_widths = []\n",
        "    distances = []\n",
        "    for pred in predictions:\n",
        "        x1, y1, x2, y2 = pred[:4]\n",
        "        class_index = int(pred[5])\n",
        "        class_label = model.names[class_index]\n",
        "        if class_label == 'motorcycle':\n",
        "          bbox_width = abs(x2 - x1)\n",
        "          bbox_widths.append(bbox_width)\n",
        "          distances.append((object_width * focal_length) / bbox_width)\n",
        "    return distances, bbox_widths\n",
        "\n",
        "# Replace 'output_frames' with the name of the folder containing frames\n",
        "input_folder = 'output_frames_d_12'\n",
        "\n",
        "# List all frame files in the input folder\n",
        "frame_files = [file for file in os.listdir(input_folder) if file.endswith('.png')]\n",
        "\n",
        "# Loop through each frame file\n",
        "for i in range(len(frame_files)):\n",
        "    # Create the full path of the image\n",
        "    image_path = os.path.join(input_folder, f'frame_{i:04d}.png')\n",
        "\n",
        "    # Open the image\n",
        "    image = Image.open(image_path)\n",
        "    bright_image= am.brighten(np.array(image), brightness_coeff=0.7)\n",
        "    # Run inference on the model\n",
        "    results = model(bright_image)\n",
        "\n",
        "    # Extract the bounding box predictions as a NumPy array\n",
        "    predictions = results.xyxy[0].numpy()\n",
        "\n",
        "    # Compute distances for each frame and print the results\n",
        "    distances, widths = compute_distance(predictions)\n",
        "    if len(distances) > 0:\n",
        "        list_of_distances_b.append(distances[0])\n",
        "        list_of_widths_b.append(widths[0])\n",
        "    else:\n",
        "      list_of_distances_b.append(np.nan)\n",
        "      list_of_widths_b.append(np.nan)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KvZg4rfVY8S8"
      },
      "source": [
        "# Rain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ir13mrvGY90n"
      },
      "outputs": [],
      "source": [
        "list_of_distances_r = []\n",
        "list_of_widths_r = []\n",
        "# Function to compute distance from bounding box\n",
        "def compute_distance(predictions):\n",
        "    focal_length = 2400.0  # Focal length in pixels\n",
        "    object_width = 0.24  # Object width in meters\n",
        "    bbox_widths = []\n",
        "    distances = []\n",
        "    for pred in predictions:\n",
        "        x1, y1, x2, y2 = pred[:4]\n",
        "        class_index = int(pred[5])\n",
        "        class_label = model.names[class_index]\n",
        "        if class_label == 'motorcycle':\n",
        "          bbox_width = abs(x2 - x1)\n",
        "          bbox_widths.append(bbox_width)\n",
        "          distances.append((object_width * focal_length) / bbox_width)\n",
        "    return distances, bbox_widths\n",
        "\n",
        "# Replace 'output_frames' with the name of the folder containing frames\n",
        "input_folder = 'output_frames_d_12'\n",
        "\n",
        "# List all frame files in the input folder\n",
        "frame_files = [file for file in os.listdir(input_folder) if file.endswith('.png')]\n",
        "\n",
        "# Loop through each frame file\n",
        "for i in range(len(frame_files)):\n",
        "    # Create the full path of the image\n",
        "    image_path = os.path.join(input_folder, f'frame_{i:04d}.png')\n",
        "\n",
        "    # Open the image\n",
        "    image = Image.open(image_path)\n",
        "    rain_image= am.add_rain(np.array(image), rain_type = 'drizzle')\n",
        "    # Run inference on the model\n",
        "    results = model(rain_image)\n",
        "\n",
        "    # Extract the bounding box predictions as a NumPy array\n",
        "    predictions = results.xyxy[0].numpy()\n",
        "\n",
        "    # Compute distances for each frame and print the results\n",
        "    distances, widths = compute_distance(predictions)\n",
        "    if len(distances) > 0:\n",
        "        list_of_distances_b.append(distances[0])\n",
        "        list_of_widths_r.append(widths[0])\n",
        "    else:\n",
        "      list_of_distances_r.append(np.nan)\n",
        "      list_of_widths_r.append(np.nan)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E6c9XyDOcsHa"
      },
      "source": [
        "# Fog"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "45vGdLSPctgy"
      },
      "outputs": [],
      "source": [
        "list_of_distances_f = []\n",
        "list_of_widths_f = []\n",
        "# Function to compute distance from bounding box\n",
        "def compute_distance(predictions):\n",
        "    focal_length = 2400.0  # Focal length in pixels\n",
        "    object_width = 0.24  # Object width in meters\n",
        "    bbox_widths = []\n",
        "    distances = []\n",
        "    for pred in predictions:\n",
        "        x1, y1, x2, y2 = pred[:4]\n",
        "        class_index = int(pred[5])\n",
        "        class_label = model.names[class_index]\n",
        "        if class_label == 'motorcycle':\n",
        "          bbox_width = abs(x2 - x1)\n",
        "          bbox_widths.append(bbox_width)\n",
        "          distances.append((object_width * focal_length) / bbox_width)\n",
        "    return distances, bbox_widths\n",
        "\n",
        "# Replace 'output_frames' with the name of the folder containing frames\n",
        "input_folder = 'output_frames_d_12'\n",
        "\n",
        "# List all frame files in the input folder\n",
        "frame_files = [file for file in os.listdir(input_folder) if file.endswith('.png')]\n",
        "\n",
        "# Loop through each frame file\n",
        "for i in range(len(frame_files)):\n",
        "    # Create the full path of the image\n",
        "    image_path = os.path.join(input_folder, f'frame_{i:04d}.png')\n",
        "\n",
        "    # Open the image\n",
        "    image = Image.open(image_path)\n",
        "    fog_image= am.add_fog(np.array(image), fog_coeff= 0.3)\n",
        "    # Run inference on the model\n",
        "    results = model(fog_image)\n",
        "\n",
        "    # Extract the bounding box predictions as a NumPy array\n",
        "    predictions = results.xyxy[0].numpy()\n",
        "\n",
        "    # Compute distances for each frame and print the results\n",
        "    distances, widths = compute_distance(predictions)\n",
        "    if len(distances) > 0:\n",
        "        list_of_distances_f.append(distances[0])\n",
        "        list_of_widths_f.append(widths[0])\n",
        "    else:\n",
        "      list_of_distances_f.append(np.nan)\n",
        "      list_of_widths_f.append(np.nan)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6PjQhGusYd6Z"
      },
      "source": [
        "# DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "idXpNg1aYdTa"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "df_distance = pd.DataFrame(list_of_distances, columns=['distance'])\n",
        "df_bbox = pd.DataFrame(list_of_widths, columns=['bbox'])\n",
        "df_distance_bright = pd.DataFrame(list_of_distances_b, columns = ['distance_bright'])\n",
        "df_bbox_bright = pd.DataFrame(list_of_widths_b, columns = ['bbox_bright'])\n",
        "df_distance_rain = pd.DataFrame(list_of_distances_r, columns = ['distance_rain'])\n",
        "df_bbox_rain = pd.DataFrame(list_of_widths_r, columns = ['bbox_rain'])\n",
        "df_distance_fog = pd.DataFrame(list_of_distances_f, columns = ['distance_fog'])\n",
        "df_bbox_fog = pd.DataFrame(list_of_widths_f, columns = ['bbox_fog'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GZBLvM7DYsJ7"
      },
      "outputs": [],
      "source": [
        "df_distance_complete = pd.concat([df_distance, df_distance_bright, df_distance_rain, df_distance_fog], axis=1)\n",
        "df_bbox_complete = pd.concat([df_bbox, df_bbox_bright, df_bbox_rain, df_bbox_fog], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "m7ytoBk9cjRO",
        "outputId": "d5d1a19c-1c60-40c1-d334-413fb122e2d5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [distance, distance_bright, distance_rain, distance_fog]\n",
              "Index: []"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-32d5a1f1-fd4f-49e8-ad12-496f48b797b8\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>distance</th>\n",
              "      <th>distance_bright</th>\n",
              "      <th>distance_rain</th>\n",
              "      <th>distance_fog</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-32d5a1f1-fd4f-49e8-ad12-496f48b797b8')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-32d5a1f1-fd4f-49e8-ad12-496f48b797b8 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-32d5a1f1-fd4f-49e8-ad12-496f48b797b8');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "df_distance_complete.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "cVl5PUT7cnkc",
        "outputId": "622ea4e0-ccd0-47d0-a00c-8c372a60b34f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [bbox, bbox_bright, bbox_rain, bbox_fog]\n",
              "Index: []"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-df2143a3-d60b-40be-9c42-65fe8800938a\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>bbox</th>\n",
              "      <th>bbox_bright</th>\n",
              "      <th>bbox_rain</th>\n",
              "      <th>bbox_fog</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-df2143a3-d60b-40be-9c42-65fe8800938a')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-df2143a3-d60b-40be-9c42-65fe8800938a button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-df2143a3-d60b-40be-9c42-65fe8800938a');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "df_bbox_complete.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "48T4gSGukpFB"
      },
      "outputs": [],
      "source": [
        "df_distance_complete.to_csv('distance_12.csv')\n",
        "df_bbox_complete.to_csv('bbox_12_drone.csv')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}